{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted Condorcet's Jury Theorem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a person who is suspected of having a hypothethical disease. He/She is tested using a machine which sends a signal $S$ depending on the precence or absence of the disease. Now, in an ideal world, the signal $S$ would have been a binary function which would have assumed one particular value for the presence of the disease and another value for the absence of the disease. However, in reality, the signal sent by the machine is actually a random variable which maybe drawn from one normal distribution if the disease is present, or from another normal distribution if the disease is absent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose the probability of the person having the disease is $p$. Therefore, the signal sent is,\n",
    "\n",
    "$$ S \\sim p \\cdot \\mathcal{N}\\left(\\mu_p, \\sigma^2\\right) + \\left(1-p\\right) \\cdot \\mathcal{N}\\left(\\mu_n, \\sigma^2\\right) $$\n",
    "\n",
    "where,\n",
    "- $p$ is the probability that the disease is present (also known as the __prior__\n",
    "- $\\mu_p$ is the mean signal if the disease is present\n",
    "- $\\mu_n$ is the mean signal if the disease is absent\n",
    "- $\\sigma$ is the standard deviation of the signal sent in either case\n",
    "- $\\mathcal{N}\\left(\\mu,\\sigma^2\\right)$ is the normal distribution with mean $\\mu$ and standard deviation $\\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \n",
    "    def __init__(self, N_exp, prior, μ_p, μ_n, σ):\n",
    "        \n",
    "        self.N_exp = N_exp\n",
    "        self.prior = prior\n",
    "        self.μ_p = μ_p\n",
    "        self.μ_n = μ_n\n",
    "        self.σ = σ\n",
    "        \n",
    "        self.event = [True if np.random.random() < prior else False for i in range(N_exp)]\n",
    "        self.signal = [μ_p + σ*(np.random.randn()) if self.event else μ_n + σ*(np.random.randn()) for i in range(N_exp)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Perfect Decision Making Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The signal $S$ is received by $N$ decision making agents (or simply, agents). Each agent $i$ decides, on the basis of the signal received, whether the patient has the disease or not. This is done on the basis of a threshold $T_i$: if the percieved signal strength is greater than the threshold $T_i$, then the agent decides that the patient has the disease; else the agent decides that the patient does not have the disease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the signal $S$ comes from normal distributions, the decisions of the agents will not always be accurate. Let $a_{p,i}$ be the probability that agent $i$ would make the correct decision if the desease is present. Similarly, let $a_{n,i}$ be the probability that agent $i$ would make the correct decision if the desease is not present. Then,\n",
    "\n",
    "$$ a_{p,i} = 1 - \\Phi\\left( T_i, \\mu_p, \\sigma \\right) $$\n",
    "and\n",
    "$$ a_{n,i} = \\Phi\\left( T_i, \\mu_n, \\sigma \\right) $$\n",
    "\n",
    "where $\\Phi\\left( x, \\mu, \\sigma \\right)$ is the value CDF of a normal distribution with mean $\\mu$ and standard deviation $\\sigma$ at point $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(x, μ_p, μ_n, σ):\n",
    "    a_p = 1 - norm.cdf(x,μ_p,σ)\n",
    "    a_n = norm.cdf(x,μ_n,σ)\n",
    "    return (a_p, a_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Matrix\n",
    "\n",
    "In order to determine the threshold $T_i$, it is important to consider the rewards of making a decision. There are four possible scenarios regarding the correctness of the decision. Each of these scenarios has its corresponding reward:\n",
    "\n",
    "$\\downarrow$ Decision/Environment $\\rightarrow$ | Positive | Negative\n",
    "------------------------------------------------|----------|----------\n",
    "Positive                                        | $R_{TP}$ | $R_{FP}$\n",
    "Negative                                        | $R_{FN}$ | $R_{TN}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Expected Reward\n",
    "\n",
    "Combing the aforementioned reward matrix, the accuracies of the agents and the priors, we get the expected reward for a given threshold to be,\n",
    "\n",
    "$$ E_i = p \\cdot \\left[ a_{p,i} \\cdot R_{TP} + \\left(1-a_{p,i} \\right) \\cdot R_{FN} \\right] + \\left[1-p \\right] \\cdot \\left[ a_{n,i} \\cdot R_{TN} + \\left(1-a_{n,i} \\right) \\cdot R_{FP} \\right] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_expected_reward(p, μ_p, μ_n, σ, R, x_min=None, x_max=None, n_points=None):\n",
    "\n",
    "    assert μ_p > μ_n\n",
    "    \n",
    "    x_min = μ_n - 6*σ if x_min == None else x_min\n",
    "    x_max = μ_p + 6*σ if x_max == None else x_max\n",
    "    n_points = 10000 if n_points == None else n_points\n",
    "    \n",
    "    assert x_max > x_min\n",
    "    \n",
    "    xs = np.linspace(x_min, x_max, n_points)\n",
    "    ys = np.zeros(len(xs))\n",
    "    \n",
    "    for i in range(len(xs)):\n",
    "        (a_p, a_n) = get_accuracy(xs[i], μ_p, μ_n, σ)\n",
    "        ys[i] = p*(a_p*R['tp']+(1-a_p)*R['fn']) + (1-p)*(a_n*R['tn']+(1-a_n)*R['fp'])\n",
    "    \n",
    "    return (xs, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the Threshold\n",
    "\n",
    "The agents choose the threshold such that the expected reward is the maximum, _i. e._,\n",
    "$$ \\dfrac{\\partial E_i}{\\partial T_i} = 0 $$\n",
    "and\n",
    "$$ \\dfrac{\\partial^2 E_i}{\\partial T_i^2} < 0. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimized_threshold(x,y):\n",
    "    \n",
    "    assert len(x) == len(y)\n",
    "    \n",
    "    r = []\n",
    "    for ii in range(1,len(xx)-1):\n",
    "        if y[ii] > y[ii+1] and y[ii] > y[ii-1]:\n",
    "            a = {'threshold': x[ii], \n",
    "                 'reward': y[ii],\n",
    "                 'index': ii\n",
    "                }\n",
    "            r.append(a)\n",
    "            \n",
    "    if len(r) == 0:\n",
    "        print('No maximas found')\n",
    "        return\n",
    "    if len(r) > 1:\n",
    "        print('Multiple maximas found')\n",
    "    \n",
    "    return r[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Imperfect Decision Maker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this decision process, the agent can make the following two types errors:\n",
    "\n",
    "- Error in percieving the ideal signal strengths $\\mu_p$ and $\\mu_n$.\n",
    "- Error in percieving sent signal strength $S$.\n",
    "\n",
    "Let us assume that the percieved ideal signal signal strengths are $\\tilde{\\mu}_{p,i}$ and $\\tilde{\\mu}_{n,i}$, and the percieved signal strength is $\\tilde{S}_i$. Then,\n",
    "\n",
    "$$ \\tilde{\\mu}_{p,i} = \\mu_p + \\epsilon_i $$\n",
    "$$ \\tilde{\\mu}_{n,i} = \\mu_n + \\epsilon_i $$\n",
    "and\n",
    "$$ \\tilde{S}_{i} = S + \\zeta_i $$\n",
    "\n",
    "where, $ \\epsilon_i \\sim \\mathcal{N} \\left( 0, e_{\\mu}^2 \\right) $ and $ \\zeta_i \\sim \\mathcal{N} \\left( 0, e_{S}^2 \\right) $ are the errors in determining the means and the signal respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, idx, p, μ_p, μ_n, σ, R, err_μ=0):\n",
    "        \n",
    "        self.index = idx\n",
    "        self.μ_p_prime = μ_p + err_μ*np.random.randn()\n",
    "        self.μ_n_prime = μ_n + err_μ*np.random.randn()\n",
    "        \n",
    "        xx, yy = get_expected_reward(p, self.μ_p_prime, self.μ_n_prime, σ, R)\n",
    "        opt = get_optimized_threshold(xx, yy)\n",
    "        self.threshold_prime = opt['threshold']\n",
    "        \n",
    "    def decide(S, err_S=0):\n",
    "        \n",
    "        self.S_prime = S + err_μ*np.random.randn()\n",
    "        self.decision = self.S_prime > self.threshold_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.5\n",
    "\n",
    "μ_p = 1\n",
    "μ_n = -1\n",
    "σ = 0.3\n",
    "\n",
    "R = {'tp': 2, 'fn': 0, 'tn': 1, 'fp': 0}\n",
    "\n",
    "N_agents = 10\n",
    "N_experiments = 1000\n",
    "\n",
    "err_μ = 0.05\n",
    "err_S = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideal Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy = (get_expected_reward(p, μ_p, μ_n, σ, R))\n",
    "ideal_parameters = get_optimized_threshold(xx, yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(N_experiments, p, μ_p, μ_n, σ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:29<00:00,  3.04s/it]\n"
     ]
    }
   ],
   "source": [
    "f = open('agents.csv', 'w')\n",
    "f.write('Index, Percieved μ_p, Percieved μ_n, Threshold\\n')\n",
    "\n",
    "agents = []\n",
    "for i in tqdm(range(10)):\n",
    "    a = Agent(i, p, μ_p, μ_n, σ, R, 0.05)\n",
    "    string = string = str(i) + ',' + str(a.μ_p_prime) + ',' + str(a.μ_n_prime) + ',' + str(a.threshold_prime) + '\\n'\n",
    "    f.write(string)\n",
    "    agents.append(a)\n",
    "\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
